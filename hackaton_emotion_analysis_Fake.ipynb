{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10716ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "from datetime import date # Removed duplicate import of datetime\n",
    "import calendar\n",
    "\n",
    "import nltk\n",
    "nltk.download() # Commented out, as it may prompt a download dialog\n",
    "\n",
    "import string\n",
    "import regex # Removed duplicate import of re\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "\n",
    "import contractions\n",
    "\n",
    "# Text to emotion \n",
    "import nltk\n",
    "nltk.download('punkt') # Moved download to correct location\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import requests\n",
    "\n",
    "from LeXmo import LeXmo\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "\n",
    "from pysummarization.nlpbase.auto_abstractor import AutoAbstractor\n",
    "from pysummarization.tokenizabledoc.simple_tokenizer import SimpleTokenizer\n",
    "from pysummarization.abstractabledoc.top_n_rank_abstractor import TopNRankAbstractor\n",
    "\n",
    "import operator\n",
    "\n",
    "# Increase display options for Pandas dataframes\n",
    "pd.options.display.max_columns = 2000\n",
    "pd.options.display.max_rows = 2000\n",
    "pd.options.display.max_colwidth = 10000\n",
    "pd.options.display.max_seq_items = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8408a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read databases\n",
    "df_news_true=pd.read_csv('data/DataSet_Misinfo_TRUE.csv',index_col=False)\n",
    "df_news_fake=pd.read_csv('data/DataSet_Misinfo_FAKE.csv')\n",
    "df_news_rpsub=pd.read_csv('data/EXTRA_RussianPropagandaSubset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3020ce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43642/43642 [04:52<00:00, 149.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 749699 sentences in the Text\n",
      "we have 526615 unique sentences in the Text\n",
      "The first sentence is:\n",
      "The Christian Post picked up the story and video interview, in which Assaf narrates the final days of her 18-year-old’s life.\n"
     ]
    }
   ],
   "source": [
    "# Separate each paragraph into sentences using nltk.sent_tokenize\n",
    "df_sentences=pd.DataFrame(columns=['sent'])\n",
    "list_token=[]\n",
    "for para in tqdm(df_news_fake.text):\n",
    "    if pd.isna(para)==False:\n",
    "        tokens = nltk.sent_tokenize(para)\n",
    "        list_token=list_token+tokens\n",
    "\n",
    "# print the number of sentences        \n",
    "print(\"we have {} sentences in the Text\".format(len(list_token)))\n",
    "\n",
    "# Print the number of unique sentences       \n",
    "print(\"we have {} unique sentences in the Text\".format(len(set(list_token))))\n",
    "\n",
    "#convert the list of sentences into dataframe \n",
    "df_sentence =pd.DataFrame([set(list_token)]).T\n",
    "\n",
    "# Rename the column \n",
    "df_sentence.columns=['sent']\n",
    "\n",
    "# Print the first sentence\n",
    "print ( \"The first sentence is:\")\n",
    "print(df_sentence.loc[1,'sent'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d03720a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155516 sent               ‘ARTIK İŞ KAPIYA GELDİKTEN SONRA MÜDAHALE DÖNEMİ BİTTİ' \\nErdoğan, ‘terörle mücadele' konusuna değinerek savunmadan taarruza geçtiklerini belirterek \"Artık tehditlerin kapımıza dayanmasını beklemeyeceğimiz.\n",
      "anger                                                                                                                                                                                                                           0\n",
      "anticipation                                                                                                                                                                                                                    0\n",
      "disgust                                                                                                                                                                                                                         0\n",
      "fear                                                                                                                                                                                                                            0\n",
      "joy                                                                                                                                                                                                                             0\n",
      "negative                                                                                                                                                                                                                        0\n",
      "positive                                                                                                                                                                                                                        0\n",
      "sadness                                                                                                                                                                                                                         0\n",
      "surprise                                                                                                                                                                                                                        0\n",
      "trust                                                                                                                                                                                                                           0\n",
      "sent_cont_fixed                                                                                                                                                                                                                  \n",
      "Name: 155516, dtype: object\n",
      "465500 sent               Birleşik Krallık, Suriye’de cihatçıları eğitmeye yeniden başlıyor Voltaire İletişim Ağı | 2 Kasım 2016 français Español Deutsch Português italiano İngiliz Savunma Bakanı Michael Fallon, ülkesinin Özgür Suriye Ordusunu yeniden oluşturacağını duyurdu.\n",
      "anger                                                                                                                                                                                                                                                                      0\n",
      "anticipation                                                                                                                                                                                                                                                               0\n",
      "disgust                                                                                                                                                                                                                                                                    0\n",
      "fear                                                                                                                                                                                                                                                                       0\n",
      "joy                                                                                                                                                                                                                                                                        0\n",
      "negative                                                                                                                                                                                                                                                                   0\n",
      "positive                                                                                                                                                                                                                                                                   0\n",
      "sadness                                                                                                                                                                                                                                                                    0\n",
      "surprise                                                                                                                                                                                                                                                                   0\n",
      "trust                                                                                                                                                                                                                                                                      0\n",
      "sent_cont_fixed                                                                                                                                                                                                                                                             \n",
      "Name: 465500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Add columns for each emotion to the df_sentence DataFrame\n",
    "df_sentence['anger']=0\n",
    "df_sentence['anticipation']=0\n",
    "df_sentence['disgust']=0\n",
    "df_sentence['fear']=0\n",
    "df_sentence['joy']=0\n",
    "df_sentence['negative']=0\n",
    "df_sentence['positive']=0\n",
    "df_sentence['sadness']=0\n",
    "df_sentence['surprise']=0\n",
    "df_sentence['trust']=0\n",
    "\n",
    "# Fix contractions like \"I'm\" to \"I am\"\n",
    "df_sentence['sent_cont_fixed']=''\n",
    "for i,row in df_sentence.iterrows():\n",
    "    # Use a try-except block to handle errors for non-English sentences\n",
    "    try:\n",
    "        df_sentence.loc[i,'sent_cont_fixed']=contractions.fix(row.sent)\n",
    "    \n",
    "    # If an error is caught, print the index and row for reference\n",
    "    except:\n",
    "        print(i,row)\n",
    "    \n",
    "# keep a copy of df_sentence\n",
    "df_sentence_new=df_sentence.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence, calculate the the related emotions and fill the values \n",
    "for i,row in tqdm_notebook(df_sentence_new.iterrows()):\n",
    "     # Use a try-except block to handle errors for null sentences\n",
    "    try:\n",
    "        emo=(LeXmo.LeXmo(row.sent_cont_fixed))\n",
    "        emo.pop('text', None)\n",
    "        for key,value in emo.items():\n",
    "            df_sentence_new.loc[i,key]=round(value,3)  \n",
    "    except:\n",
    "        print(i,row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62850c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00579753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the database with the emotion for each sentence\n",
    "df_sentence_new.to_hdf('data/df_news_fake_emotions.h5', key='df_news_fake_emotions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c855029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b670b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11977f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
